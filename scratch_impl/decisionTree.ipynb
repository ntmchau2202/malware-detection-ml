from random import seed
from random import randrange
import pandas as pd
import numbers
import logging
import tensorflow as tf

def loadCsv(path):
  # file = open(filename, "rt")
  lines = pd.read_csv(path, sep='|')
  data = lines.drop(['Name', 'md5', 'legitimate'], axis=1).values
  dataset = list(data)
  return data

# Convert string column to float
def str_column_to_float(dataset, column):
  for row in dataset:
    if isinstance(row, numbers.Number):
      row[column] = float(row[column].strip())

# Split a dataset into k folds
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split
 
# Calculate accuracy percentage
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0
 
# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
  folds = cross_validation_split(dataset, n_folds)
  scores = list()
  for i in range(len(folds)):
    train_set = list(folds)
    train_set.pop(i)
    train_set = sum(train_set, [])
    test_set = list()
    for row in folds[i]:
      row_copy = list(row)
      test_set.append(row_copy)
      row_copy[-1] = None
    predicted = algorithm(train_set, test_set, *args)
    actual = [row[-1] for row in folds[i]]
    accuracy = accuracy_metric(actual, predicted)
    scores.append(accuracy)
  return scores

# This is used to calculate the Gini index, given groups and classes
# In this malware classification projects, initially we have 1 group (?) and 2 classes (malware and benign)
def giniIndex(groups, classes):
  # count all number of instance in all input groups
  numInstance = (sum([len(group) for group in groups]))
  gini = 0
  for group in groups:
    size = float(len(group))
    # avoid divide by 0
    if size == 0:
      continue
    score = 0
    # score the group based on score of each class
    for classVal in classes:
      p = [row[-1] for row in group].count(classVal) / size
      score += p * p
		# weight the group score by its relative size
    # gini_index = (1.0 - sum(proportion * proportion)) * (group_size/total_samples)
    gini += (1.0 - score) * (size / numInstance)
  return gini

# this is used to split the original dataset into 2 set, based on the input value; index is the index of the attribute on the table
def trySplit(index, value, dataset):
  left, right = list(), list()
  for row in dataset:
      if row[index] < value:
        left.append(row)
      else:
        right.append(row)
  return left, right

# this function gives us the best split for data based on all atributes
def getSplit(dataset):
  # initialize all class values. In the dataset, the last value is the classification (0,1)
  classValues = list(set(row[-1] for row in dataset))
  bIdx, bValue, bScore, bGroups = 9999, 9999, 9999, [list(), list()]
  # we will start consider each attributes in the given dataset
  for index in range(len(dataset[0])-1):
    # consider each record
    for row in dataset:
      # split the dataset based on the considered attribute
      # this will generate a left group and right group based on the given (middle value)
      groups = trySplit(index, row[index], dataset)
      # this split attempt will give us a gini score. The smaller gini score the better
      gini = giniIndex(groups, classValues)
      if gini < bScore:
        bIdx, bValue, bScore, bGroups = index, row[index], gini, groups
  return {'index': bIdx, 'value': bValue, 'groups': bGroups}

# this function selects a class value for each group of rows, which is the MOST COMMON VALUE in a list of row
def terminate(group):
  outcome = [row[-1] for row in group]
  # i.e if there are more 1 then return 1 and vice versa
  return max(set(outcome), key=outcome.count)

# Building tree
# Idea: calling getSplit over and over for the given each group at each node
# Steps for building a tree
# 1. At the node, extract 2 groups of data and delete them for node (the node no longer need it for use)
# 2. Check if the either left or right node is empty. If it is, create a terminal for the left/right node, using the left/right record
# 3. Process the left child:
#         - If the group of rows is too small, create a terminal node
#         - Else, depth-first create and add left node until the bottom of the tree is reached
# 4. Do the same manner with the right child

def grow(node, maxDepth, minSize, depth):
  left, right = node['groups']
  del node['groups']
  # 1.
  if not left or not right:
    node['left'] = node['right'] = terminate(left + right)
    return
  # Check for max depth
  if depth >= maxDepth:
    node['left'], node['right'] = terminate(left), terminate(right)
    return
  # 3.
  if len(left) <= minSize:
    node['left'] = terminate(left)
  else:
    node['left'] = getSplit(left)
    grow(node['left'], maxDepth, minSize, depth+1)
  #4.
  if len(right) <= minSize:
    node['right'] = terminate(right)
  else:
    node['right'] = getSplit(right)
    grow(node['right'], maxDepth, minSize, depth+1)

def buildTree(train, maxDepth, minSize):
  root = getSplit(train)
  grow(root, maxDepth, minSize, 1)
  return root

# Prediction
# Idea: navigate the tree, turn left and right accordingly with each attribute to reach the terminal
def predict(node, row):
  if row[node['index']] < node['value']:
    if isinstance(node['left'], dict):
      # there is something to search
      return predict(node['left'], row)
    else:
      # we have reached the terminal
      return node['left']
  else:
    if isinstance(node['right'], dict):
      return predict(node['right'], row)
    else:
      return node['right']

# Classification and Regression Tree Algorithm
def decisionTree(train, test, max_depth, min_size):
	tree = buildTree(train, max_depth, min_size)
	predictions = list()
	for row in test:
		prediction = predict(tree, row)
		predictions.append(prediction)
	return(predictions)


deviceName = tf.test.gpu_device_name()
with tf.device(deviceName):
  logging.basicConfig(format='%(gmttime)s - %(message)s', level=logging.INFO)

  linkFile = 'https://raw.githubusercontent.com/ntmchau2202/malware-detection-ml/master/resources/data.csv'
  # Test CART on Bank Note dataset
  seed(1)
  # load and prepare data
  dataset = loadCsv(linkFile)
  logging.info(len(dataset))


  # # convert string attributes to integers
  for i in range(len(dataset[0])):
    str_column_to_float(dataset, i)
  # evaluate algorithm
  n_folds = 5
  for max_depth in range(5, 11):
    for min_size in range(6, 12): 
      scores = evaluate_algorithm(dataset, decisionTree, n_folds, max_depth, min_size)
      logging.info("Done one sprint")
      print("=======================")
      print("Depth::Minsize", max_depth, ":", min_size)
      print('Scores: %s' % scores)
      print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))

  logging.info("Done training")
